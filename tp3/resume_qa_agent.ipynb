{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7215dd06",
   "metadata": {},
   "source": [
    "# Resume Q&A Agent\n",
    "\n",
    "This notebook implements a multi-agent system using LangGraph to answer questions about candidate resumes. The system uses a Pinecone vector database to store and retrieve resume information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad781731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import SecretStr\n",
    "from pinecone import Pinecone\n",
    "from langgraph.types import Send\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.documents import Document\n",
    "from typing import Annotated, List, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d9a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment and API Configuration ---\n",
    "\n",
    "# Load API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY or not PINECONE_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"Please set your OPENAI_API_KEY and PINECONE_API_KEY environment \" \n",
    "        \"variables.\"\n",
    "    )\n",
    "\n",
    "# Configure the LLM and Embeddings model\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'gpt-4o')\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n",
    "EMBEDDING_DIMENSIONS = int(os.getenv('EMBEDDING_DIMENSIONS', 768))\n",
    "\n",
    "# Initialize the LLM for generation\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.1,\n",
    "    api_key=SecretStr(OPENAI_API_KEY)\n",
    ")\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    dimensions=EMBEDDING_DIMENSIONS,\n",
    "    api_key=SecretStr(OPENAI_API_KEY)\n",
    ")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713e43e",
   "metadata": {},
   "source": [
    "### Schemas and State\n",
    "\n",
    "We define the data structures that will be used to manage the state of our graph.\n",
    "- **`Candidates`**: A Pydantic model to structure the output of our planner, which identifies candidate names from the user's question.\n",
    "- **`State`**: The main state of the graph, tracking the question, identified candidates, retrieved contexts, and the final answer.\n",
    "- **`WorkerState`**: The state passed to each parallel worker, containing the specific candidate and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab2941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for identifying candidates in the user's query\n",
    "class Candidates(BaseModel):\n",
    "    names: List[str] = Field(\n",
    "        description=\"A list of candidate names mentioned in the user's query.\",\n",
    "    )\n",
    "\n",
    "# LLM augmented with the schema for structured output\n",
    "planner = llm.with_structured_output(Candidates)\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    candidates: List[str]\n",
    "    context: Annotated[List[Document], operator.add]\n",
    "    answer: str\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    question: str\n",
    "    candidate: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8978354",
   "metadata": {},
   "source": [
    "### Graph Nodes\n",
    "\n",
    "These are the core functions that will be executed as nodes in our graph.\n",
    "- **`orchestrator`**: The entry point that uses an LLM to identify the candidates mentioned in the question.\n",
    "- **`retrieval_worker`**: The worker function that runs in parallel for each candidate. It connects to the appropriate Pinecone index, retrieves relevant resume chunks, and formats them as context.\n",
    "- **`responder`**: The final node that synthesizes the retrieved contexts from all workers into a single, coherent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe78679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    \"\"\"\n",
    "    Identifies candidates from the user's question and updates the state.\n",
    "    \"\"\"\n",
    "    print(\"Orchestrator: Identifying candidates...\")\n",
    "    question = state['question']    \n",
    "    # Use the planner to extract candidate names\n",
    "    candidate_model = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are an expert at extracting candidate names from a \"\n",
    "                    \"user query.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    \"Extract the full names of any candidates mentioned in the \"\n",
    "                    f\"following query: '{question}'\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )    \n",
    "    candidates = candidate_model.names\n",
    "    print(f\"Orchestrator: Found candidates: {candidates}\")    \n",
    "    return {\"candidates\": candidates}\n",
    "\n",
    "\n",
    "def retrieval_worker(state: WorkerState):\n",
    "    \"\"\"\n",
    "    Retrieves context for a single candidate from their Pinecone index.\n",
    "    \"\"\"\n",
    "    candidate = state['candidate']\n",
    "    question = state['question']    \n",
    "    print(f\"Worker: Retrieving context for {candidate}...\")    \n",
    "    # Sanitize candidate name to create a valid index name\n",
    "    index_name = f\"{candidate.lower().replace(' ', '-')}-index\"    \n",
    "    try:\n",
    "        # Ensure the index exists before trying to use it\n",
    "        if index_name not in pc.list_indexes().names():\n",
    "            print(f\"Worker: Index '{index_name}' not found. Skipping.\")\n",
    "            return {\"context\": []}\n",
    "        # Create a vector store instance\n",
    "        vector_store = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "        )        \n",
    "        # Retrieve relevant documents\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\n",
    "                'k': 3,\n",
    "                'score_threshold': 0.35\n",
    "            }\n",
    "        )\n",
    "        retrieved_docs = retriever.invoke(question)        \n",
    "        print(f\"Worker: Successfully retrieved context for {candidate}.\")\n",
    "        return {\"context\": retrieved_docs}\n",
    "    except Exception as e:\n",
    "        print(f\"Worker: Error retrieving context for {candidate}: {e}\")\n",
    "        return {\"context\": []}\n",
    "\n",
    "\n",
    "def responder(state: State):\n",
    "    \"\"\"\n",
    "    Generates the final answer based on the retrieved contexts.\n",
    "    \"\"\"\n",
    "    print(\"Responder: Generating final answer...\")\n",
    "    question = state['question']\n",
    "    context = \"\".join(d.page_content for d in state['context'])    \n",
    "    # Generate the final response\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant for a resume Q&A system. \"\n",
    "        \"Based on the provided context from one or more resumes, answer the \"\n",
    "        \"user's question accurately. \"\n",
    "        \"If the context does not contain the answer, state that the \"\n",
    "        \"information is not available in the resume(s).\"\n",
    "    )    \n",
    "    human_message = f\"Question: {question}\\n\\nContext:\\n{context}\"    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=human_message)\n",
    "    ])    \n",
    "    print(\"Responder: Done.\")\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc452d9",
   "metadata": {},
   "source": [
    "### Graph Definition and Execution\n",
    "\n",
    "Here, we build the graph by defining the nodes and the edges that connect them.\n",
    "- The graph starts with the `orchestrator`.\n",
    "- A conditional edge, `assign_workers`, dynamically creates parallel `retrieval_worker` tasks for each identified candidate.\n",
    "- The `responder` node is called after all workers have completed.\n",
    "- Finally, the graph is compiled and is ready to be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9a490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edge function to assign work to parallel workers\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"\n",
    "    Assigns a worker to each identified candidate.\n",
    "    \"\"\"\n",
    "    print(\"Assigning work to workers...\")\n",
    "    # The `Send` tool allows us to trigger multiple parallel node executions\n",
    "    return [\n",
    "        Send(\"retrieval_worker\", {\"candidate\": candidate, \"question\": state['question']})\n",
    "        for candidate in state['candidates']\n",
    "    ]\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"orchestrator\", orchestrator)\n",
    "graph_builder.add_node(\"retrieval_worker\", retrieval_worker)\n",
    "graph_builder.add_node(\"responder\", responder)\n",
    "\n",
    "# Define edges\n",
    "graph_builder.add_edge(START, \"orchestrator\")\n",
    "graph_builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"retrieval_worker\"])\n",
    "graph_builder.add_edge(\"retrieval_worker\", \"responder\")\n",
    "graph_builder.add_edge(\"responder\", END)\n",
    "\n",
    "# Compile the graph\n",
    "resume_agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0594841",
   "metadata": {},
   "source": [
    "### Running the Agent\n",
    "\n",
    "Now you can ask questions about the resumes. The agent will identify the candidates, retrieve the relevant information from Pinecone, and generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b8ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator: Identifying candidates...\n",
      "Orchestrator: Found candidates: ['John Doe']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for John Doe...\n",
      "Orchestrator: Found candidates: ['John Doe']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for John Doe...\n",
      "Worker: Successfully retrieved context for John Doe.\n",
      "Responder: Generating final answer...\n",
      "Worker: Successfully retrieved context for John Doe.\n",
      "Responder: Generating final answer...\n",
      "Responder: Done.\n",
      "Responder: Done.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes. John Doe worked at Microsoft Corporation as a Senior Software Engineer from January 2023 to August 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Example 1: Question about a single candidate ---\n",
    "question1 = \"Did John Doe work at Microsoft?\"\n",
    "result1 = resume_agent.invoke({\"question\": question1})\n",
    "\n",
    "Markdown(result1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7926949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator: Identifying candidates...\n",
      "Orchestrator: Found candidates: ['John Doe', 'Mark Nash']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for John Doe...\n",
      "Worker: Retrieving context for Mark Nash...\n",
      "Orchestrator: Found candidates: ['John Doe', 'Mark Nash']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for John Doe...\n",
      "Worker: Retrieving context for Mark Nash...\n",
      "Worker: Successfully retrieved context for John Doe.\n",
      "Worker: Successfully retrieved context for John Doe.\n",
      "Worker: Successfully retrieved context for Mark Nash.\n",
      "Responder: Generating final answer...\n",
      "Worker: Successfully retrieved context for Mark Nash.\n",
      "Responder: Generating final answer...\n",
      "Responder: Done.\n",
      "Responder: Done.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "- John Doe: Microsoft Corporation (Senior Software Engineer), United States\n",
       "- Mark Nash: DeepMind Technologies (Senior Machine Learning Engineer), United Kingdom"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Example 2: Question about multiple candidates ---\n",
    "question2 = \"Where did John Doe and Mark Nash work in August 2025?\"\n",
    "result2 = resume_agent.invoke({\"question\": question2})\n",
    "\n",
    "Markdown(result2['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918d3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator: Identifying candidates...\n",
      "Orchestrator: Found candidates: ['Elon Musk']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for Elon Musk...\n",
      "Orchestrator: Found candidates: ['Elon Musk']\n",
      "Assigning work to workers...\n",
      "Worker: Retrieving context for Elon Musk...\n",
      "Worker: Index 'elon-musk-index' not found. Skipping.\n",
      "Responder: Generating final answer...\n",
      "Worker: Index 'elon-musk-index' not found. Skipping.\n",
      "Responder: Generating final answer...\n",
      "Responder: Done.\n",
      "Responder: Done.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The information is not available in the provided resume(s)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Example 3: Question about a candidate not in the system ---\n",
    "question3 = \"What is Elon Musk's experience?\"\n",
    "result3 = resume_agent.invoke({\"question\": question3,})\n",
    "\n",
    "Markdown(result3['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
